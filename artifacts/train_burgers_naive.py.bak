import numpy as np
import torch
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

# This repo
from pathlib import Path
import sys
sys.path.append(str(Path(__file__).resolve().parent / 'network' ))
sys.path.append(str(Path(__file__).resolve().parent / 'data'))

import burgers_dataset
from unet_1d import cstam

###############################################
# Create dataloader around our dataset object #
###############################################
train_dataset = burgers_dataset.Dataset()
train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=0) 

###############################################
# Create Unet and optimizer to solve params   #
###############################################
model = cstam()
#model.load_state_dict(torch.load('scale_naive_1.pth'))  # load a state dictionary for the model
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=.00005)
loss_fcn = torch.nn.MSELoss()

###############################################
# Optimize the Unet model parameters          #
###############################################
retrain = True
if retrain:
  losses = []
  residuals = []
  # Loop over the dataset
  for epoch in range(3):
    for i, data in enumerate(train_loader, 0):
      u0, U = data             # b, p are batch x t x c x l
      optimizer.zero_grad()
      preds = model(u0, 1, train_dataset.dt, train_dataset.N_t)
      loss = loss_fcn(preds, U)
      loss.backward()
      print( 'Epoch: {}, Iteration: {}, loss: {}'.format(epoch, i, loss.item() ) )
      optimizer.step()
      losses.append(loss.item())

      # Change domain discretization before next iteration
      #new_Nx = 2**np.random.randint(9,12)
      new_Nx = 128 * np.random.randint(3,5)
      new_Nt = np.random.randint(30, 50)
      train_dataset.domain_change(new_Nx,new_Nt)

    PATH = 'scale_naive_{}.pth'.format(epoch + 1)
    torch.save(model.state_dict(), PATH)
  
###############################################
# Plot errors for different dx, dt values?    #
###############################################
# Try 8-13 (256-8192 dx steps)
# Try 20-100 for different T steps?
# y-axis=mean-error, x-axis=N_x 

test_dataset = burgers_dataset.Dataset(cnt=100,test=True)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0) 

Nxs = np.array([128*xx for xx in range(2,9)])
Nts = np.array( list(range(20,70,10)) )
all_errs = np.zeros((Nxs.shape[0], Nts.shape[0]))

for ii, new_Nx in enumerate(Nxs):
  for jj, new_Nt in enumerate(Nts):
    # Configure test dataset
    test_dataset.domain_change(new_Nx, new_Nt)
    #test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True, num_workers=0)

    # Go through test dataset
    with torch.no_grad():
      errs = []
      for i, data in enumerate(test_loader, 0):
        u0, U = data             # b, p are batch x t x c x l
        preds = model(u0, 1, test_dataset.dt, test_dataset.N_t)
        loss = loss_fcn(preds, U)
        errs.append(loss.item())
      
      # Average errors
      all_errs[ii,jj] = np.mean(errs)
  

np.save('all_errs.npy', all_errs)
all_Nts, all_Nxs = np.meshgrid(Nts, Nxs)
data = np.stack([all_Nxs, all_Nts, all_errs],axis=2).reshape(-1,3)
import pandas as pd
df = pd.DataFrame(data,columns=['Nx','Nt','err'])
df['in_domain'] = (df['Nx']>=384)*(df['Nx']<=512)*(df['Nt']>=30)*(df['Nt']<=50)
df['dx'] = 10.0 / df['Nx']
df['dt'] = 1.0 / df['Nt']

# Plot better
for ii in range(Nxs.shape[0]):
  # Plot all dts as striped
  sub_df = df[df['Nx']==Nxs[i]]
  # Plot over with solid anything that is in the training domain


# Plots...
for ii in range(Nxs.shape[0]):
  plt.plot(Nts, all_errs[ii,:], label='N_x={}'.format(Nxs[ii]))
plt.legend()
plt.ylabel('Mean squared error')
plt.xlabel('Number of time steps')
plt.show()

